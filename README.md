# ELSA

The Code is based on the official code of [ELSA](https://github.com/sIncerass/ELSA) but with some fixes.

ELSA is an emoji-powered representation learning framework for cross-lingual sentiment classification. 

The workflow of ELSA consists of the following phases:

1. It uses large-scale Tweets to learn word embeddings through Word2Vec of both the source and the target languages in an unsupervised way. 

2. In a distant-supervised way, it uses emojis as complementary sentiment labels to transform the word embeddings into a higher-level sentence representation that encodes rich sentiment information via an emoji-prediction task through an attention-based stacked bi-directional LSTM model. This step is also conducted separately for both the source and the target languages. 

3. It translates the labeled English data into the target language through an off-the-shelf machine translation system, represent the pseudo parallel texts with the pre-trained language-specific models, and use an attention model to train the final sentiment classifier.

You can see the WWW 2019 (know as The Web Conference) paper “**Emoji-Powered Representation Learning for Cross-Lingual Sentiment Classification**” for more details.

## Dataset
### Sentiment representation learning
Large-scale dataset of Tweets for training sentiment representation models.

### Cross lingual sentiment analysis
dataset/Amazon review/ contains the pre-processed Amazon review dataset created by [Prettenhofer and Stein](http://www.aclweb.org/anthology/P10-1114). Aside from the given parallel texts of the test data (i.e., the Japanese, French and German reviews), we translate English reviews into Japanese, French, and German through [Google Translate](https://translate.google.com). Each line of these included files is composed of `sentiment label \t english version \t other language version`, please use the json files to parse each review (e.g., `./en_de/de/books_train_review.tsv`, they have already been processed into list of words)

## Setup
Dockerfile is located at docker/ with all dependencies.

## Emoji-Powered Representation Learning
Extract text from tweets json collected using twitter streaming API or twitterscraper.
```
python ./script/extract_text.py --lang <L1> --lang <L2> <TWEETS_JSON> <DATA_DIR>
```

Run preprocessing scripts below in sequence. Tweets containing the top 64 emojis is used to train. As many Tweets contain multiple emojis but task is designed as single-label classification task predicting only the first emoji appeared instead of more complicated multi-label classification. ( I tried multi-label classification but there was no noticeable improvement)
```
# Tokenize extracted tweet text.
python ./script/tweet_token.py <DATA_DIR> <L1 or L2>
# Train word embedding.
python ./script/tidy_wordvec.py <DATA_DIR> <L1 or L2>
# Tidy vocabulary
python ./script/tidy_vocab.py <DATA_DIR> <L1 or L2>
# Generate training data for sentence representation learning (emoji prediction).
python ./script/tidy_tweet_elsa.py <DATA_DIR> <L1 or L2>
```

Train sentence representation model by predicting emoji in sentence. After training emoji-prediction model ELSA uses the intermediate layer output of the model as a sentence encoder. 
```
python ./elsa/train_elsa_sentence.py --lang <L1 or L2>
```

## Training and testing the Sentiment Classifier
Given sentences in the target language, its in the source language (usually English) was generated by off-the-shelf machine translation system in advance. Amazon Review dataset was made as well. ELSA uses the intermediate layer output of the emoji prediction model as a sentence embedding.  For evaluation we use the labeled English data for training and other languages for the testing. Below is an example targeting Japanese book reviews. First we outputs embeddings of both the source and the target sentences which are used as inputs for sentiment classifier training.
```
python elsa/encode_elsa_sentence.py --data ./dataset/Amazon\ review/en_jp/en/books_train_review.csv --data_dir /data/elsa --s_weight ./ckpt/elsa_en.hdf5 --t_weight ./ckpt/elsa_ja.hdf5 --s_maxlen 20 --t_maxlen 50
```

The value of maxlen for each language must be set the value which estimated by the tidy_tweet_elsa.py, it's usually 50 for Japanese and 20 for others.

ELSA aggregate these sentence representations back to form two compact representations for each training document, one in English and the other in the target language. ELSA use the two representations as features to predict the real sentiment label of each document and obtain the final sentiment classifier.
Training the sentiment classifier on the labeled English data.
```
python ./elsa/train_elsa_doc.py --data ./embed/books_train_review.npz --s_lang en --s_maxlen 20 --t_lang ja --t_maxlen 50
```

In the test phase, for a new document in the target language, we translate it into English then follow the previous steps to obtain its representation, based on which we predict the sentiment label using the classifier. 

Encode the test data as well as the training data. The source language is set Japanese here (a bit confusing ?).
```
python elsa/encode_elsa_sentence.py --data ./dataset/Amazon\ review/en_jp/jp/books_test_review.csv --data_dir /data/elsa --s_weight ./ckpt/elsa_ja.hdf5 --t_weight ./ckpt/elsa_en.hdf5 --s_maxlen 50 --t_maxlen 20
```

Finally we evaluate the classifier.
```
python ./elsa/train_elsa_doc.py --data ./embed/books_test_review.npz --s_lang en --s_maxlen 20 --t_lang ja --t_maxlen 50 --test
```
