{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import keras\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "import yaml\n",
    "\n",
    "from attlayer import AttentionWeightedAverage\n",
    "#from avglayer import MaskAverage\n",
    "from copy import deepcopy\n",
    "#from finetuning import (sampling_generator, finetuning_callbacks)\n",
    "from operator import itemgetter\n",
    "#from global_variables import NB_TOKENS, NB_EMOJI_CLASSES\n",
    "from keras.layers import *\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Input, Bidirectional, Embedding, Dense, Dropout, SpatialDropout1D, LSTM, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import L1L2 \n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, recall_score, precision_score, f1_score\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elsa_architecture(nb_classes, nb_tokens, maxlen, feature_output=False, embed_dropout_rate=0, final_dropout_rate=0, embed_dim=300,\n",
    "                    embed_l2=1E-6, return_attention=False, load_embedding=False, pre_embedding=None, high=False, LSTM_hidden=512, LSTM_drop=0.5):\n",
    "    \"\"\"\n",
    "    Returns the DeepMoji architecture uninitialized and\n",
    "    without using the pretrained model weights.\n",
    "    # Arguments:\n",
    "        nb_classes: Number of classes in the dataset.\n",
    "        nb_tokens: Number of tokens in the dataset (i.e. vocabulary size).\n",
    "        maxlen: Maximum length of a token.\n",
    "        feature_output: If True the model returns the penultimate\n",
    "                        feature vector rather than Softmax probabilities\n",
    "                        (defaults to False).\n",
    "        embed_dropout_rate: Dropout rate for the embedding layer.\n",
    "        final_dropout_rate: Dropout rate for the final Softmax layer.\n",
    "        embed_l2: L2 regularization for the embedding layerl.\n",
    "        high: use or not the highway network\n",
    "    # Returns:\n",
    "        Model with the given parameters.\n",
    "    \"\"\"\n",
    "    class NonMasking(Layer):   \n",
    "        def __init__(self, **kwargs):   \n",
    "            self.supports_masking = True  \n",
    "            super(NonMasking, self).__init__(**kwargs)   \n",
    "\n",
    "        def build(self, input_shape):   \n",
    "            input_shape = input_shape   \n",
    "\n",
    "        def compute_mask(self, input, input_mask=None):   \n",
    "            # do not pass the mask to the next layers   \n",
    "            return None   \n",
    "\n",
    "        def call(self, x, mask=None):   \n",
    "            return x   \n",
    "\n",
    "        def get_output_shape_for(self, input_shape):   \n",
    "            return input_shape \n",
    "    # define embedding layer that turns word tokens into vectors\n",
    "    # an activation function is used to bound the values of the embedding\n",
    "    model_input = Input(shape=(maxlen,), dtype='int32')\n",
    "    embed_reg = L1L2(l2=embed_l2) if embed_l2 != 0 else None\n",
    "\n",
    "    if not load_embedding and pre_embedding is None:\n",
    "        embed = Embedding(input_dim=nb_tokens, output_dim=embed_dim, mask_zero=True,input_length=maxlen,embeddings_regularizer=embed_reg,\n",
    "                          name='embedding')\n",
    "    else:\n",
    "        embed = Embedding(input_dim=nb_tokens, output_dim=embed_dim, mask_zero=True,input_length=maxlen, weights=[pre_embedding],\n",
    "                          embeddings_regularizer=embed_reg,trainable=True, name='embedding')\n",
    "    if high:\n",
    "        x = NonMasking()(embed(model_input))\n",
    "    else:\n",
    "        x = embed(model_input)\n",
    "    x = Activation('tanh')(x)\n",
    "\n",
    "    # entire embedding channels are dropped out instead of the\n",
    "    # normal Keras embedding dropout, which drops all channels for entire words\n",
    "    # many of the datasets contain so few words that losing one or more words can alter the emotions completely\n",
    "    if embed_dropout_rate != 0:\n",
    "        embed_drop = SpatialDropout1D(embed_dropout_rate, name='embed_drop')\n",
    "        x = embed_drop(x)\n",
    "\n",
    "    # skip-connection from embedding to output eases gradient-flow and allows access to lower-level features\n",
    "    # ordering of the way the merge is done is important for consistency with the pretrained model\n",
    "    lstm_0_output = Bidirectional(LSTM(LSTM_hidden, return_sequences=True, dropout=LSTM_drop), name=\"bi_lstm_0\" )(x)\n",
    "    lstm_1_output = Bidirectional(LSTM(LSTM_hidden, return_sequences=True, dropout=LSTM_drop), name=\"bi_lstm_1\" )(lstm_0_output)\n",
    "    x = concatenate([lstm_1_output, lstm_0_output, x])\n",
    "    if high:\n",
    "        x = TimeDistributed(Highway(activation='tanh', name=\"high\"))(x)\n",
    "    # if return_attention is True in AttentionWeightedAverage, an additional tensor\n",
    "    # representing the weight at each timestep is returned\n",
    "    weights = None\n",
    "    x = AttentionWeightedAverage(name='attlayer', return_attention=return_attention)(x)\n",
    "    #x = MaskAverage(name='attlayer', return_attention=return_attention)(x)\n",
    "    if return_attention:\n",
    "        x, weights = x\n",
    "\n",
    "    if not feature_output:\n",
    "        # output class probabilities\n",
    "        if final_dropout_rate != 0:\n",
    "            x = Dropout(final_dropout_rate)(x)\n",
    "\n",
    "        if nb_classes > 2:\n",
    "            outputs = [Dense(nb_classes, activation='softmax', name='softmax')(x)]\n",
    "        else:\n",
    "            outputs = [Dense(1, activation='sigmoid', name='softmax')(x)]\n",
    "    else:\n",
    "        # output penultimate feature vector\n",
    "        outputs = [x]\n",
    "\n",
    "    if return_attention:\n",
    "        # add the attention weights to the outputs if required\n",
    "        outputs.append(weights)\n",
    "\n",
    "    return Model(inputs=[model_input], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "cur_lan = \"elsa_de\"\n",
    "maxlen = 20\n",
    "batch_size = 250\n",
    "lr = 3e-4\n",
    "epoch_size = 25000\n",
    "nb_epochs = 1000\n",
    "patience = 5\n",
    "checkpoint_weight_path = \"./ckpt\"\n",
    "loss = \"categorical_crossentropy\"\n",
    "optim = \"adam\"\n",
    "vocab_path = \"/data/elsa\"\n",
    "nb_classes=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_hidden = 512\n",
    "LSTM_drop = 0.5\n",
    "final_dropout_rate = 0.5\n",
    "embed_dropout_rate = 0.0\n",
    "high = False\n",
    "load_embedding = True\n",
    "embed_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = int(epoch_size/batch_size)\n",
    "\n",
    "wv_path = Path(vocab_path).joinpath(\"{:s}_wv.npy\".format(cur_lan)).as_posix()\n",
    "X_path = Path(vocab_path).joinpath(\"{:s}_X.npy\".format(cur_lan)).as_posix()\n",
    "y_path = Path(vocab_path).joinpath(\"{:s}_y.npy\".format(cur_lan)).as_posix()\n",
    "\n",
    "word_vec = np.load(wv_path, allow_pickle=True)\n",
    "input_vec, input_label = np.load(X_path, allow_pickle=True), np.load(y_path, allow_pickle=True)\n",
    "nb_tokens, input_len = len(word_vec), len(input_label)\n",
    "\n",
    "train_end = int(input_len*0.7)\n",
    "val_end = int(input_len*0.9)\n",
    "\n",
    "(X_train, y_train) = (input_vec[:train_end], input_label[:train_end])\n",
    "(X_val, y_val) = (input_vec[train_end:val_end], input_label[train_end:val_end])\n",
    "(X_test, y_test) = (input_vec[val_end:], input_label[val_end:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 20, 200)      7720600     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 200)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_0 (Bidirectional)       (None, 20, 1024)     2920448     activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_1 (Bidirectional)       (None, 20, 1024)     6295552     bi_lstm_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 20, 2248)     0           bi_lstm_1[0][0]                  \n",
      "                                                                 bi_lstm_0[0][0]                  \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attlayer (AttentionWeightedAver (None, 2248)         2248        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2248)         0           attlayer[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Dense)                 (None, 64)           143936      dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 17,082,784\n",
      "Trainable params: 17,082,784\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = elsa_architecture(nb_classes=nb_classes, nb_tokens=nb_tokens, maxlen=maxlen, final_dropout_rate=final_dropout_rate, embed_dropout_rate=embed_dropout_rate, \n",
    "                          load_embedding=True, pre_embedding=word_vec, high=high, embed_dim=embed_dim)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 32351 samples, validate on 9243 samples\n",
      "Epoch 1/1000\n",
      "32351/32351 [==============================] - 55s 2ms/step - loss: 3.7845 - acc: 0.1313 - val_loss: 3.5956 - val_acc: 0.1457\n",
      "Epoch 2/1000\n",
      "32351/32351 [==============================] - 32s 982us/step - loss: 3.4940 - acc: 0.1645 - val_loss: 3.3770 - val_acc: 0.1773\n",
      "Epoch 3/1000\n",
      "32351/32351 [==============================] - 33s 1ms/step - loss: 3.3188 - acc: 0.1895 - val_loss: 3.2876 - val_acc: 0.1944\n",
      "Epoch 4/1000\n",
      "32351/32351 [==============================] - 30s 924us/step - loss: 3.2093 - acc: 0.2065 - val_loss: 3.2570 - val_acc: 0.2000\n",
      "Epoch 5/1000\n",
      "32351/32351 [==============================] - 30s 930us/step - loss: 3.1203 - acc: 0.2212 - val_loss: 3.2324 - val_acc: 0.2062\n",
      "Epoch 6/1000\n",
      "32351/32351 [==============================] - 32s 989us/step - loss: 3.0375 - acc: 0.2356 - val_loss: 3.2185 - val_acc: 0.2074\n",
      "Epoch 7/1000\n",
      "32351/32351 [==============================] - 32s 977us/step - loss: 2.9529 - acc: 0.2519 - val_loss: 3.2235 - val_acc: 0.2063\n",
      "Epoch 8/1000\n",
      "32351/32351 [==============================] - 32s 982us/step - loss: 2.8674 - acc: 0.2723 - val_loss: 3.2533 - val_acc: 0.2042\n",
      "Epoch 9/1000\n",
      "32351/32351 [==============================] - 31s 965us/step - loss: 2.7815 - acc: 0.2857 - val_loss: 3.3048 - val_acc: 0.1970\n",
      "Epoch 10/1000\n",
      "32351/32351 [==============================] - 30s 942us/step - loss: 2.6891 - acc: 0.3050 - val_loss: 3.3516 - val_acc: 0.1968\n",
      "Epoch 11/1000\n",
      "32351/32351 [==============================] - 32s 983us/step - loss: 2.5978 - acc: 0.3255 - val_loss: 3.3863 - val_acc: 0.1936ss: 2.5991 - acc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3cc80d2b38>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if optim == 'adam':\n",
    "    adam = Adam(clipnorm=1, lr=lr)\n",
    "    model.compile(loss=loss, optimizer=adam, metrics=['accuracy'])\n",
    "elif optim == 'rmsprop':\n",
    "    model.compile(loss=loss, optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=nb_epochs,\n",
    "          validation_data=(X_val, y_val),\n",
    "          callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=patience, verbose=0, mode='auto')],\n",
    "          verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19363911691706504\n"
     ]
    }
   ],
   "source": [
    "_, acc = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2index = json.loads(open(\"/data/elsa/elsa_de_vocab.txt\", \"r\").read())\n",
    "\n",
    "freq = {line.split()[0]: int(line.split()[1]) for line in open(\"/data/elsa/elsa_de_emoji.txt\").readlines()}\n",
    "freq_topn = sorted(freq.items(), key=itemgetter(1), reverse=True)[:nb_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           ğŸ˜‚       0.22      0.55      0.31       599\n",
      "           ğŸ˜       0.20      0.24      0.21       233\n",
      "           â¤       0.12      0.17      0.14       183\n",
      "           ğŸ‘       0.16      0.27      0.20       166\n",
      "           ğŸ˜Š       0.12      0.21      0.15       207\n",
      "           ğŸ¤£       0.12      0.01      0.01       136\n",
      "           ğŸ˜‰       0.09      0.18      0.12       210\n",
      "           ğŸ˜˜       0.16      0.22      0.19       123\n",
      "           ğŸ¤”       0.19      0.28      0.23       192\n",
      "           ğŸ˜­       0.23      0.22      0.22       114\n",
      "           ğŸ˜       0.02      0.01      0.01       149\n",
      "           â˜•       0.26      0.57      0.36        65\n",
      "           ğŸ˜…       0.04      0.01      0.02       134\n",
      "           ğŸ˜       0.05      0.03      0.04        98\n",
      "           ğŸ™„       0.11      0.03      0.05       115\n",
      "           â­       0.68      0.66      0.67        67\n",
      "           ğŸ™ˆ       0.00      0.00      0.00        87\n",
      "           â™‚       0.19      0.25      0.21        12\n",
      "           ğŸ¤—       0.00      0.00      0.00        72\n",
      "           â™€       0.00      0.00      0.00        12\n",
      "           ğŸ€       0.10      0.18      0.13        34\n",
      "           ğŸ¤·       0.55      0.76      0.64        85\n",
      "           ğŸ”¥       0.26      0.20      0.23        49\n",
      "           â˜€       0.17      0.05      0.07        43\n",
      "           ğŸ˜€       0.00      0.00      0.00        60\n",
      "           ğŸ’•       0.25      0.03      0.05        37\n",
      "           ğŸ˜       0.00      0.00      0.00        60\n",
      "           ğŸ¥°       0.00      0.00      0.00        47\n",
      "           ğŸ˜±       0.17      0.02      0.04        50\n",
      "           ğŸ‘       0.14      0.04      0.06        27\n",
      "           ğŸ‘Œ       0.00      0.00      0.00        41\n",
      "           ğŸ˜œ       0.00      0.00      0.00        46\n",
      "           ğŸ™       0.33      0.10      0.16        39\n",
      "           ğŸ’ª       0.17      0.07      0.10        41\n",
      "           ğŸ¤¦       0.48      0.23      0.31        44\n",
      "           ğŸ˜³       0.00      0.00      0.00        50\n",
      "           ğŸ˜¡       0.15      0.07      0.10        42\n",
      "           ğŸ‘‹       0.44      0.12      0.20        32\n",
      "           â˜º       0.00      0.00      0.00        47\n",
      "           ğŸ™‹       0.53      0.56      0.54        34\n",
      "           ğŸ˜„       0.00      0.00      0.00        47\n",
      "           ğŸ’–       0.43      0.11      0.17        28\n",
      "           â˜       0.10      0.08      0.09        48\n",
      "           ğŸ˜‹       0.00      0.00      0.00        34\n",
      "           ğŸ’œ       0.00      0.00      0.00        27\n",
      "           ğŸ˜”       0.00      0.00      0.00        44\n",
      "           ğŸŒ       0.00      0.00      0.00        21\n",
      "           â™¥       0.00      0.00      0.00        31\n",
      "           ğŸ¥º       0.17      0.03      0.05        37\n",
      "           ğŸ’š       0.00      0.00      0.00        26\n",
      "           ğŸ™‚       0.00      0.00      0.00        42\n",
      "           ğŸ¤ª       0.00      0.00      0.00        36\n",
      "           ğŸ˜ƒ       0.00      0.00      0.00        33\n",
      "           ğŸ‘€       0.50      0.03      0.05        35\n",
      "           âœŒ       0.00      0.00      0.00        24\n",
      "           ğŸ’™       0.00      0.00      0.00        21\n",
      "           ğŸ¤®       0.33      0.09      0.14        23\n",
      "           ğŸ‰       0.23      0.27      0.25        26\n",
      "           ğŸ’‹       0.80      0.20      0.32        20\n",
      "           ğŸ˜†       0.00      0.00      0.00        28\n",
      "           ğŸ˜¬       0.00      0.00      0.00        32\n",
      "           ğŸ™ƒ       0.00      0.00      0.00        30\n",
      "           ğŸ˜‡       0.00      0.00      0.00        26\n",
      "           âœ¨       0.00      0.00      0.00        21\n",
      "\n",
      "    accuracy                           0.19      4622\n",
      "   macro avg       0.14      0.11      0.11      4622\n",
      "weighted avg       0.15      0.19      0.15      4622\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1), target_names=[e[0] for e in freq_topn]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
