{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json, os, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Input, Bidirectional, Embedding, Dense, Dropout, SpatialDropout1D, LSTM, Activation\n",
    "from keras.regularizers import L1L2\n",
    "from attlayer import AttentionWeightedAverage\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, classification_report\n",
    "from keras.models import load_model\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elsa_architecture(nb_classes, nb_tokens, maxlen, feature_output=False, embed_dropout_rate=0, final_dropout_rate=0, embed_dim=300,\n",
    "                    embed_l2=1E-6, return_attention=False, load_embedding=False, pre_embedding=None, high=False, test=False, LSTM_drop=0.5, LSTM_hidden=512):\n",
    "    \"\"\"\n",
    "    Returns the DeepMoji architecture uninitialized and\n",
    "    without using the pretrained model weights.\n",
    "    # Arguments:\n",
    "        nb_classes: Number of classes in the dataset.\n",
    "        nb_tokens: Number of tokens in the dataset (i.e. vocabulary size).\n",
    "        maxlen: Maximum length of a token.\n",
    "        feature_output: If True the model returns the penultimate\n",
    "                        feature vector rather than Softmax probabilities\n",
    "                        (defaults to False).\n",
    "        embed_dropout_rate: Dropout rate for the embedding layer.\n",
    "        final_dropout_rate: Dropout rate for the final Softmax layer.\n",
    "        embed_l2: L2 regularization for the embedding layerl.\n",
    "        high: use or not the highway network\n",
    "    # Returns:\n",
    "        Model with the given parameters.\n",
    "    \"\"\"\n",
    "    class NonMasking(Layer):   \n",
    "        def __init__(self, **kwargs):   \n",
    "            self.supports_masking = True  \n",
    "            super(NonMasking, self).__init__(**kwargs)   \n",
    "\n",
    "        def build(self, input_shape):   \n",
    "            input_shape = input_shape   \n",
    "\n",
    "        def compute_mask(self, input, input_mask=None):   \n",
    "            # do not pass the mask to the next layers   \n",
    "            return None   \n",
    "\n",
    "        def call(self, x, mask=None):   \n",
    "            return x   \n",
    "\n",
    "        def get_output_shape_for(self, input_shape):   \n",
    "            return input_shape \n",
    "    # define embedding layer that turns word tokens into vectors\n",
    "    # an activation function is used to bound the values of the embedding\n",
    "    model_input = Input(shape=(maxlen,), dtype='int32')\n",
    "    embed_reg = L1L2(l2=embed_l2) if embed_l2 != 0 else None\n",
    "    if not load_embedding and pre_embedding is None:\n",
    "        embed = Embedding(input_dim=nb_tokens,output_dim=embed_dim, mask_zero=True,input_length=maxlen,embeddings_regularizer=embed_reg,\n",
    "                          name='embedding')\n",
    "    else:\n",
    "        embed = Embedding(input_dim=nb_tokens, output_dim=embed_dim, mask_zero=True,input_length=maxlen, weights=[pre_embedding],\n",
    "                          embeddings_regularizer=embed_reg,trainable=True, name='embedding')\n",
    "    if high:\n",
    "        x = NonMasking()(embed(model_input))\n",
    "    else:\n",
    "        x = embed(model_input)\n",
    "    x = Activation('tanh')(x)\n",
    "\n",
    "    # entire embedding channels are dropped out instead of the\n",
    "    # normal Keras embedding dropout, which drops all channels for entire words\n",
    "    # many of the datasets contain so few words that losing one or more words can alter the emotions completely\n",
    "    if not test and embed_dropout_rate != 0:\n",
    "        embed_drop = SpatialDropout1D(embed_dropout_rate, name='embed_drop')\n",
    "        x = embed_drop(x)\n",
    "\n",
    "    # skip-connection from embedding to output eases gradient-flow and allows access to lower-level features\n",
    "    # ordering of the way the merge is done is important for consistency with the pretrained model\n",
    "    lstm_0_output = Bidirectional(LSTM(LSTM_hidden, return_sequences=True, dropout=0.0 if test else LSTM_drop), name=\"bi_lstm_0\" )(x)\n",
    "    lstm_1_output = Bidirectional(LSTM(LSTM_hidden, return_sequences=True, dropout=0.0 if test else LSTM_drop), name=\"bi_lstm_1\" )(lstm_0_output)\n",
    "    x = concatenate([lstm_1_output, lstm_0_output, x])\n",
    "    if high:\n",
    "        x = TimeDistributed(Highway(activation='tanh', name=\"high\"))(x)\n",
    "    # if return_attention is True in AttentionWeightedAverage, an additional tensor\n",
    "    # representing the weight at each timestep is returned\n",
    "    weights = None\n",
    "    x = AttentionWeightedAverage(name='attlayer', return_attention=return_attention)(x)\n",
    "    #x = MaskAverage(name='attlayer', return_attention=return_attention)(x)\n",
    "    if return_attention:\n",
    "        x, weights = x\n",
    "\n",
    "    if not feature_output:\n",
    "        # output class probabilities\n",
    "        if not test and final_dropout_rate != 0:\n",
    "            x = Dropout(final_dropout_rate)(x)\n",
    "\n",
    "        if nb_classes > 2:\n",
    "            outputs = [Dense(nb_classes, activation='softmax', name='softmax')(x)]\n",
    "        else:\n",
    "            outputs = [Dense(1, activation='sigmoid', name='softmax')(x)]\n",
    "    else:\n",
    "        # output penultimate feature vector\n",
    "        outputs = [x]\n",
    "\n",
    "    if return_attention:\n",
    "        # add the attention weights to the outputs if required\n",
    "        outputs.append(weights)\n",
    "\n",
    "    return Model(inputs=[model_input], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_test(model, vocab_path, cur_lan):\n",
    "    input_vec, input_label = np.load(vocab_path + \"%s_input.npy\" % cur_lan), np.load(vocab_path + \"%s_labels.npy\" % cur_lan)\n",
    "    nb_tokens, input_len = len(word_vec), len(input_label)\n",
    "    (X_test, y_true) = (input_vec[int(input_len*0.9):], input_label[int(input_len*0.9):])\n",
    "    y_test = model.predict(X_test)\n",
    "    def top_n_accuracy(preds, truths, n):\n",
    "        best_n = np.argsort(preds, axis=1)[:,-n:]\n",
    "        ts = np.argmax(truths, axis=1)\n",
    "        print(best_n)\n",
    "        successes = 0\n",
    "        for i in range(ts.shape[0]):\n",
    "            if ts[i] in best_n[i,:]:\n",
    "                successes += 1\n",
    "        return float(successes)/ts.shape[0]\n",
    "    topk = top_n_accuracy(y_true, y_test, n=5)\n",
    "    predicted_categories = [np.argmax(x) for x in y_test]\n",
    "    expected_categories = [np.argmax(x) for x in y_true]\n",
    "    print(\"topk: \", topk)\n",
    "    print(classification_report(expected_categories, predicted_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2\"\n",
    "\n",
    "cur_lan = \"de\"\n",
    "cur_test = \"en_de/\"\n",
    "weight_path = \"./ckpt/weights.06-3.218_de.hdf5\"\n",
    "input_dir = \"../dataset/Amazon review/\"\n",
    "output_dir = \"./embed/\"\n",
    "vocab_path = \"/data/elsa/\"\n",
    "\n",
    "vocab_index = json.loads(open(vocab_path + \"elsa_de_vocab.txt\", \"r\").read())\n",
    "word_vec = np.load(vocab_path + \"elsa_de_wv.npy\")\n",
    "nb_tokens = len(word_vec)\n",
    "maxlen = 20\n",
    "embed_dim = 200\n",
    "\n",
    "batch_size = 32\n",
    "nb_classes = 64\n",
    "cur_en_or_ot = True\n",
    "test=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 20, 200)      7720600     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 200)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_0 (Bidirectional)       (None, 20, 1024)     2920448     activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_1 (Bidirectional)       (None, 20, 1024)     6295552     bi_lstm_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 20, 2248)     0           bi_lstm_1[0][0]                  \n",
      "                                                                 bi_lstm_0[0][0]                  \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attlayer (AttentionWeightedAver (None, 2248)         2248        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 16,938,848\n",
      "Trainable params: 16,938,848\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = elsa_architecture(nb_classes=nb_classes, nb_tokens=nb_tokens, maxlen=maxlen, embed_dim=embed_dim, feature_output=False, return_attention=False, test=test)\n",
    "model.load_weights(weight_path, by_name=True)\n",
    "\n",
    "intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer('attlayer').output)\n",
    "intermediate_layer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tokens(words):\n",
    "    assert len(words) > 0\n",
    "    tokens = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            tokens.append(vocab_index[w])\n",
    "        except KeyError:\n",
    "            tokens.append(1)\n",
    "    return tokens\n",
    "\n",
    "if cur_en_or_ot:\n",
    "    cur_config_lan = 'en'\n",
    "else:\n",
    "    cur_config_lan = cur_lan\n",
    "\n",
    "embed_files = os.listdir(input_dir+cur_test+'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en ../dataset/Amazon review/en_de/de/books_test_review.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dda3b30f29b4a84bd85de7c3e479295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "en ../dataset/Amazon review/en_de/de/books_train_review.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67898dcdd30c4ebf87ffca6d25356da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "en ../dataset/Amazon review/en_de/de/dvd_test_review.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6adaed699e43408f572f154a3ce690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "en ../dataset/Amazon review/en_de/de/dvd_train_review.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd7f7e8e31148d4814db3aa011ee7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "en ../dataset/Amazon review/en_de/de/music_test_review.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88b30119b214368839f04c423e38de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "en ../dataset/Amazon review/en_de/de/music_train_review.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40caf76b077409daf6d30306db72b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "en ../dataset/Amazon review/en_de/en/books_test_review.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437693ea657b4b1eae71a45e9bf8ed42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "en ../dataset/Amazon review/en_de/en/books_train_review.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a70f07f7ad14ae7aedb6a48231ebcdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "en ../dataset/Amazon review/en_de/en/dvd_test_review.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c1dc916a3041f1be42c22893e2018e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "en ../dataset/Amazon review/en_de/en/dvd_train_review.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c72c4b07cba44ecac8093be18e83568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "en ../dataset/Amazon review/en_de/en/music_test_review.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a337bc6b7f477ebbccd785dc2551f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "en ../dataset/Amazon review/en_de/en/music_train_review.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05be0dd378fc42babc63e6c9ccf184bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for embed_chose in range(2):\n",
    "    embed_file = \"en\" if embed_chose else cur_test[-3:-1]\n",
    "    for cur_file in embed_files:\n",
    "        doc_embedding, doc_file = [], []\n",
    "        next_insert = 0\n",
    "        print(cur_config_lan, input_dir+cur_test+embed_file+\"/\"+cur_file)\n",
    "        cur_ = open(input_dir+cur_test+embed_file+\"/\"+cur_file, \"r\")\n",
    "        for data in cur_:\n",
    "            line = data.strip().split('\\t')\n",
    "            if cur_config_lan != \"en\":\n",
    "                data = line[2] if embed_chose else line[1]\n",
    "            else:\n",
    "                data = line[1] if embed_chose else line[2]\n",
    "            data = json.loads(data)\n",
    "            tokens = np.zeros((len(data), maxlen), dtype='uint32')\n",
    "            next_insert = 0\n",
    "            for s_words in data:\n",
    "                s_tokens = find_tokens(s_words)\n",
    "                if len(s_tokens) > maxlen:\n",
    "                    s_tokens = s_tokens[:maxlen]\n",
    "                tokens[next_insert,:len(s_tokens)] = s_tokens\n",
    "                next_insert += 1\n",
    "            doc_file.append(tokens)\n",
    "        cur_.close()\n",
    "\n",
    "        for embed_sen in tqdm(doc_file):\n",
    "            encoding = intermediate_layer_model.predict(embed_sen)\n",
    "            doc_embedding.append(encoding)\n",
    "\n",
    "        np.save(output_dir+cur_test+cur_config_lan+\"/\"+embed_file+\"_\"+cur_file.replace(\".tsv\", \"_embed.npz\"), doc_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
