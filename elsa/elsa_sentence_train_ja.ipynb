{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from absl import flags\n",
    "from pathlib import Path\n",
    "from operator import itemgetter\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import model as nn\n",
    "importlib.reload(nn)\n",
    "\n",
    "elsa_architecture = nn.elsa_architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "lang = \"ja\"\n",
    "batch_size = 250\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "patience = 3\n",
    "data_dir = \"/data/elsa2\"\n",
    "checkpoint_dir = \"./ckpt\"\n",
    "optimizer = \"adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_hidden = 512\n",
    "lstm_drop = 0.5\n",
    "final_drop = 0.5\n",
    "embed_drop = 0.0\n",
    "highway = False\n",
    "compute_class_weight = False\n",
    "multilabel = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 200)      65831000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 50, 200)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_0 (Bidirectional)       (None, 50, 1024)     2920448     activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_1 (Bidirectional)       (None, 50, 1024)     6295552     bi_lstm_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 50, 2248)     0           bi_lstm_1[0][0]                  \n",
      "                                                                 bi_lstm_0[0][0]                  \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attlayer (AttentionWeightedAver (None, 2248)         2248        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2248)         0           attlayer[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Dense)                 (None, 64)           143936      dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 75,193,184\n",
      "Trainable params: 75,193,184\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 1721192 samples, validate on 491770 samples\n",
      "Epoch 1/100\n",
      "1721192/1721192 [==============================] - 3327s 2ms/step - loss: 2.9393 - acc: 0.2650 - val_loss: 2.8071 - val_acc: 0.2904\n",
      "Epoch 2/100\n",
      "1721192/1721192 [==============================] - 3407s 2ms/step - loss: 2.7767 - acc: 0.2991 - val_loss: 2.7781 - val_acc: 0.2992\n",
      "Epoch 3/100\n",
      "1721192/1721192 [==============================] - 3459s 2ms/step - loss: 2.7120 - acc: 0.3150 - val_loss: 2.7888 - val_acc: 0.3013\n",
      "Epoch 4/100\n",
      "  68000/1721192 [>.............................] - ETA: 50:20 - loss: 2.5287 - acc: 0.3582"
     ]
    }
   ],
   "source": [
    "data_dir = Path(data_dir)\n",
    "wv_path = (data_dir / \"{:s}_wv.npy\".format(lang)).__str__()\n",
    "X_path = (data_dir / \"{:s}_X.npy\".format(lang)).__str__()\n",
    "y_path = (data_dir / \"{:s}_y.npy\".format(lang)).__str__()\n",
    "emoji_path = (data_dir / \"{:s}_emoji.txt\".format(lang)).__str__()\n",
    "\n",
    "wv = np.load(wv_path, allow_pickle=True)\n",
    "input_vec = np.load(X_path, allow_pickle=True)\n",
    "input_label = np.load(y_path, allow_pickle=True)\n",
    "\n",
    "nb_tokens = len(wv)\n",
    "embed_dim = wv.shape[1]\n",
    "input_len = len(input_label)\n",
    "nb_classes = input_label.shape[1]\n",
    "maxlen = input_vec.shape[1]\n",
    "\n",
    "train_end = int(input_len*0.7)\n",
    "val_end = int(input_len*0.9)\n",
    "\n",
    "(X_train, y_train) = (input_vec[:train_end], input_label[:train_end])\n",
    "(X_val, y_val) = (input_vec[train_end:val_end], input_label[train_end:val_end])\n",
    "(X_test, y_test) = (input_vec[val_end:], input_label[val_end:])\n",
    "\n",
    "if multilabel:\n",
    "    def to_multilabel(y):\n",
    "        outputs = []\n",
    "        for i in nb_classes:\n",
    "            outputs.append(y[:, i])\n",
    "        return outputs\n",
    "\n",
    "    y_train = to_multilabel(y_train)\n",
    "    y_val = to_multilabel(y_val)\n",
    "    y_test = to_multilabel(y_test)\n",
    "\n",
    "model = elsa_architecture(nb_classes=nb_classes,\n",
    "                          nb_tokens=nb_tokens,\n",
    "                          maxlen=maxlen,\n",
    "                          final_dropout_rate=final_drop,\n",
    "                          embed_dropout_rate=embed_drop,\n",
    "                          load_embedding=True,\n",
    "                          pre_embedding=wv,\n",
    "                          high=highway,\n",
    "                          embed_dim=embed_dim,\n",
    "                          multilabel=multilabel)\n",
    "model.summary()\n",
    "\n",
    "computed_class_weight = None\n",
    "\n",
    "if multilabel:\n",
    "    loss = \"binary_crossentropy\"\n",
    "else:\n",
    "    loss = \"categorical_crossentropy\"\n",
    "    if compute_class_weight:\n",
    "        y_train_sps = []\n",
    "        for row in y_train:\n",
    "            y_train_sps.extend(np.where(row)[0].tolist())\n",
    "        computed_class_weight = class_weight.compute_class_weight(\n",
    "            'balanced', list(range(nb_classes)), y_train_sps)\n",
    "        print(\"computed class weight = {:s}\".format(str(computed_class_weight)))\n",
    "\n",
    "if optimizer == 'adam':\n",
    "    adam = Adam(clipnorm=1, lr=lr)\n",
    "    model.compile(loss=loss, optimizer=adam, metrics=['accuracy'])\n",
    "elif optimizer == 'rmsprop':\n",
    "    model.compile(loss=loss, optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "checkpoint_dir = Path(checkpoint_dir)\n",
    "if not checkpoint_dir.exists():\n",
    "    checkpoint_dir.mkdir()\n",
    "checkpoint_weight_path = (checkpoint_dir / \"elsa_{:s}.hdf5\".format(lang)).__str__()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', min_delta=0, patience=patience, verbose=0, mode='auto'),\n",
    "    keras.callbacks.ModelCheckpoint(checkpoint_weight_path, monitor='val_loss',\n",
    "                                    verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "]\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_val, y_val),\n",
    "          class_weight=computed_class_weight,\n",
    "          callbacks=callbacks,\n",
    "          verbose=1)\n",
    "\n",
    "freq = {line.split()[0]: int(line.split()[1]) for line in open(emoji_path).readlines()}\n",
    "freq_topn = sorted(freq.items(), key=itemgetter(1), reverse=True)[:nb_classes]\n",
    "\n",
    "if multilabel:\n",
    "    y_pred = model.predict([X_test], batch_size=batch_size)\n",
    "    y_pred = [np.squeeze(p) for p in y_pred]\n",
    "\n",
    "    y_test_1d = np.array(y_test).flatten()\n",
    "    y_pred_1d = np.array(y_pred).flatten()\n",
    "    print(f1_score(y_test_1d, y_pred_1d > 0.5))\n",
    "    print(classification_report(y_test_1d, y_pred_1d > 0.5))\n",
    "\n",
    "    gold, pred = [], []\n",
    "    for i in range(len(X_test)):\n",
    "        each_gold, each_pred = [], []\n",
    "        for c in range(nb_classes):\n",
    "            if y_test[c][i] == 1.0:\n",
    "                each_gold.append(c+1)\n",
    "            else:\n",
    "                each_gold.append(0)\n",
    "            if y_pred[c][i] > 0.5:\n",
    "                each_pred.append(c+1)\n",
    "            else:\n",
    "                each_pred.append(0)\n",
    "        gold.extend(each_gold)\n",
    "        pred.extend(each_pred)\n",
    "\n",
    "    target_name = [\"\"] + [e[0] for e in freq_topn]\n",
    "    print(classification_report(gold, pred, target_names=target_name))\n",
    "else:\n",
    "    _, acc = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
    "    print(acc)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test.argmax(axis=1), y_pred.argmax(\n",
    "        axis=1), target_names=[e[0] for e in freq_topn]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
